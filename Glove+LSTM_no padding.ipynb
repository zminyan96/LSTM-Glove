{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38fb44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "# import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a1ae53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c3fa4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "413a90f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\muz94\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stop_word = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "406dc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train/train.csv\", dtype={'id': int, 'qid1':int, 'qid2': int, 'question1':str, 'question2':str, 'is_duplicate': int})\n",
    "train_data.set_index('id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bb01826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199481</th>\n",
       "      <td>300983</td>\n",
       "      <td>300984</td>\n",
       "      <td>How much initial capital investment would I re...</td>\n",
       "      <td>How much initial capital investment would I re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190458</th>\n",
       "      <td>289543</td>\n",
       "      <td>289544</td>\n",
       "      <td>What exactly is the Arab Spring?</td>\n",
       "      <td>What is Arab Spring?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286653</th>\n",
       "      <td>407280</td>\n",
       "      <td>407281</td>\n",
       "      <td>Why should people switch to LED bulbs?</td>\n",
       "      <td>What should I pay attention to when buying LED...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40544</th>\n",
       "      <td>60811</td>\n",
       "      <td>11501</td>\n",
       "      <td>How do I tell my religious family I don't beli...</td>\n",
       "      <td>How do I tell my parent I don't believe in God?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46093</th>\n",
       "      <td>82485</td>\n",
       "      <td>82486</td>\n",
       "      <td>Is using essay writing services ethical?</td>\n",
       "      <td>Are essay writing services ethical?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38603</th>\n",
       "      <td>70083</td>\n",
       "      <td>70084</td>\n",
       "      <td>How does RSA decryption work?</td>\n",
       "      <td>Can you answer my RSA question?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51363</th>\n",
       "      <td>5988</td>\n",
       "      <td>12951</td>\n",
       "      <td>What is the expected KVPY 2016 SA cut off?</td>\n",
       "      <td>What do you think about the KVPY 2016 paper?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230732</th>\n",
       "      <td>143875</td>\n",
       "      <td>131004</td>\n",
       "      <td>Would Modi \"dare\" to abolish the reservation?</td>\n",
       "      <td>Will PM Modi abolish caste based reservations?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384803</th>\n",
       "      <td>45489</td>\n",
       "      <td>125363</td>\n",
       "      <td>Which is the best laptop for gaming under 60k ...</td>\n",
       "      <td>Which is best gaming laptop under 60000?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382867</th>\n",
       "      <td>514761</td>\n",
       "      <td>514762</td>\n",
       "      <td>When were cotton fibres first used?</td>\n",
       "      <td>Where was cotton first used?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36924 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid1    qid2                                          question1  \\\n",
       "id                                                                          \n",
       "199481  300983  300984  How much initial capital investment would I re...   \n",
       "190458  289543  289544                   What exactly is the Arab Spring?   \n",
       "286653  407280  407281             Why should people switch to LED bulbs?   \n",
       "40544    60811   11501  How do I tell my religious family I don't beli...   \n",
       "46093    82485   82486           Is using essay writing services ethical?   \n",
       "...        ...     ...                                                ...   \n",
       "38603    70083   70084                      How does RSA decryption work?   \n",
       "51363     5988   12951         What is the expected KVPY 2016 SA cut off?   \n",
       "230732  143875  131004      Would Modi \"dare\" to abolish the reservation?   \n",
       "384803   45489  125363  Which is the best laptop for gaming under 60k ...   \n",
       "382867  514761  514762                When were cotton fibres first used?   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "id                                                                       \n",
       "199481  How much initial capital investment would I re...             1  \n",
       "190458                               What is Arab Spring?             1  \n",
       "286653  What should I pay attention to when buying LED...             1  \n",
       "40544     How do I tell my parent I don't believe in God?             1  \n",
       "46093                 Are essay writing services ethical?             1  \n",
       "...                                                   ...           ...  \n",
       "38603                     Can you answer my RSA question?             1  \n",
       "51363        What do you think about the KVPY 2016 paper?             1  \n",
       "230732     Will PM Modi abolish caste based reservations?             1  \n",
       "384803           Which is best gaming laptop under 60000?             1  \n",
       "382867                       Where was cotton first used?             1  \n",
       "\n",
       "[36924 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_data.sample(n=100000).copy()\n",
    "data.loc[data['is_duplicate']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a2e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen2list(sentence: str):\n",
    "    mod_lst = []\n",
    "    try:\n",
    "        sentence = clean_text(sentence)\n",
    "\n",
    "        lst = word_tokenize(sentence)\n",
    "        \n",
    "        #ps = PorterStemmer()\n",
    "        for word in lst:\n",
    "            c = word[0]\n",
    "            if word != \"\" and ((\"a\"<=c and c <=\"z\") or (\"A\" <= c and c <=\"Z\") or (\"0\"<= c and c <=\"9\")):\n",
    "                #mod_word = ps.stem(word)\n",
    "                mod_word = word\n",
    "                if mod_word not in stop_word:\n",
    "                    mod_lst.append(mod_word)\n",
    "        \n",
    "    except:\n",
    "        print(sentence)\n",
    "    \n",
    "    return mod_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f220f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "data['lst1'] = data.apply(lambda row: sen2list(row.question1), axis = 1)\n",
    "data['lst2'] = data.apply(lambda row: sen2list(row.question2), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff787a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>lst1</th>\n",
       "      <th>lst2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191740</th>\n",
       "      <td>291166</td>\n",
       "      <td>291167</td>\n",
       "      <td>Do we really need an election commission in In...</td>\n",
       "      <td>Is election commission of India poorly framed?</td>\n",
       "      <td>0</td>\n",
       "      <td>[really, need, election, commission, India]</td>\n",
       "      <td>[election, commission, India, poorly, framed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89303</th>\n",
       "      <td>150062</td>\n",
       "      <td>150063</td>\n",
       "      <td>What is the right way to say \"you're welcome\" ...</td>\n",
       "      <td>What is the right way to say \"live free\" in It...</td>\n",
       "      <td>0</td>\n",
       "      <td>[right, way, say, welcome, italian]</td>\n",
       "      <td>[right, way, say, live, free, italian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124380</th>\n",
       "      <td>200861</td>\n",
       "      <td>200862</td>\n",
       "      <td>How could Russia hack US election?</td>\n",
       "      <td>Can the US redo the 2016 Presidential election...</td>\n",
       "      <td>0</td>\n",
       "      <td>[could, russia, hack, america, election]</td>\n",
       "      <td>[america, redo, 2016, presidential, election, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199481</th>\n",
       "      <td>300983</td>\n",
       "      <td>300984</td>\n",
       "      <td>How much initial capital investment would I re...</td>\n",
       "      <td>How much initial capital investment would I re...</td>\n",
       "      <td>1</td>\n",
       "      <td>[much, initial, capital, investment, would, re...</td>\n",
       "      <td>[much, initial, capital, investment, would, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190458</th>\n",
       "      <td>289543</td>\n",
       "      <td>289544</td>\n",
       "      <td>What exactly is the Arab Spring?</td>\n",
       "      <td>What is Arab Spring?</td>\n",
       "      <td>1</td>\n",
       "      <td>[exactly, arab, spring]</td>\n",
       "      <td>[arab, spring]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328666</th>\n",
       "      <td>455240</td>\n",
       "      <td>455241</td>\n",
       "      <td>What is the most touching real life love story...</td>\n",
       "      <td>What are some examples of patents simple enoug...</td>\n",
       "      <td>0</td>\n",
       "      <td>[touching, real, life, love, story, come, across]</td>\n",
       "      <td>[examples, patents, simple, enough, beginner, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384803</th>\n",
       "      <td>45489</td>\n",
       "      <td>125363</td>\n",
       "      <td>Which is the best laptop for gaming under 60k ...</td>\n",
       "      <td>Which is best gaming laptop under 60000?</td>\n",
       "      <td>1</td>\n",
       "      <td>[best, laptop, gaming, 60000, inr]</td>\n",
       "      <td>[best, gaming, laptop, 60000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134515</th>\n",
       "      <td>214973</td>\n",
       "      <td>214974</td>\n",
       "      <td>What are potato bugs and are they harmful to h...</td>\n",
       "      <td>How harmful could it be to have bugs in the re...</td>\n",
       "      <td>0</td>\n",
       "      <td>[potato, bugs, harmful, humans]</td>\n",
       "      <td>[harmful, could, bugs, refrigerator]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382867</th>\n",
       "      <td>514761</td>\n",
       "      <td>514762</td>\n",
       "      <td>When were cotton fibres first used?</td>\n",
       "      <td>Where was cotton first used?</td>\n",
       "      <td>1</td>\n",
       "      <td>[cotton, fibres, first, used]</td>\n",
       "      <td>[cotton, first, used]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79939</th>\n",
       "      <td>136010</td>\n",
       "      <td>136011</td>\n",
       "      <td>How is the word \"indomitable\" used in a sentence?</td>\n",
       "      <td>What are career opportunities after entering i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[word, indomitable, used, sentence]</td>\n",
       "      <td>[career, opportunities, entering, fmcg, sales,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99958 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid1    qid2                                          question1  \\\n",
       "id                                                                          \n",
       "191740  291166  291167  Do we really need an election commission in In...   \n",
       "89303   150062  150063  What is the right way to say \"you're welcome\" ...   \n",
       "124380  200861  200862                 How could Russia hack US election?   \n",
       "199481  300983  300984  How much initial capital investment would I re...   \n",
       "190458  289543  289544                   What exactly is the Arab Spring?   \n",
       "...        ...     ...                                                ...   \n",
       "328666  455240  455241  What is the most touching real life love story...   \n",
       "384803   45489  125363  Which is the best laptop for gaming under 60k ...   \n",
       "134515  214973  214974  What are potato bugs and are they harmful to h...   \n",
       "382867  514761  514762                When were cotton fibres first used?   \n",
       "79939   136010  136011  How is the word \"indomitable\" used in a sentence?   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "id                                                                        \n",
       "191740     Is election commission of India poorly framed?             0   \n",
       "89303   What is the right way to say \"live free\" in It...             0   \n",
       "124380  Can the US redo the 2016 Presidential election...             0   \n",
       "199481  How much initial capital investment would I re...             1   \n",
       "190458                               What is Arab Spring?             1   \n",
       "...                                                   ...           ...   \n",
       "328666  What are some examples of patents simple enoug...             0   \n",
       "384803           Which is best gaming laptop under 60000?             1   \n",
       "134515  How harmful could it be to have bugs in the re...             0   \n",
       "382867                       Where was cotton first used?             1   \n",
       "79939   What are career opportunities after entering i...             0   \n",
       "\n",
       "                                                     lst1  \\\n",
       "id                                                          \n",
       "191740        [really, need, election, commission, India]   \n",
       "89303                 [right, way, say, welcome, italian]   \n",
       "124380           [could, russia, hack, america, election]   \n",
       "199481  [much, initial, capital, investment, would, re...   \n",
       "190458                            [exactly, arab, spring]   \n",
       "...                                                   ...   \n",
       "328666  [touching, real, life, love, story, come, across]   \n",
       "384803                 [best, laptop, gaming, 60000, inr]   \n",
       "134515                    [potato, bugs, harmful, humans]   \n",
       "382867                      [cotton, fibres, first, used]   \n",
       "79939                 [word, indomitable, used, sentence]   \n",
       "\n",
       "                                                     lst2  \n",
       "id                                                         \n",
       "191740      [election, commission, India, poorly, framed]  \n",
       "89303              [right, way, say, live, free, italian]  \n",
       "124380  [america, redo, 2016, presidential, election, ...  \n",
       "199481  [much, initial, capital, investment, would, re...  \n",
       "190458                                     [arab, spring]  \n",
       "...                                                   ...  \n",
       "328666  [examples, patents, simple, enough, beginner, ...  \n",
       "384803                      [best, gaming, laptop, 60000]  \n",
       "134515               [harmful, could, bugs, refrigerator]  \n",
       "382867                              [cotton, first, used]  \n",
       "79939   [career, opportunities, entering, fmcg, sales,...  \n",
       "\n",
       "[99958 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d9145e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[(data['lst1'].str.len() > 0) & (data['lst2'].str.len() > 0),:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a23eec",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e89315ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f420deed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199916"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = data['lst1'].tolist() + data['lst2'].tolist()\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c50b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45864"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(corpus)\n",
    "len(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "037dd6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 50\n",
    "def text_to_seq(lst: list) -> list:\n",
    "    if len(lst) <= max_seq_len:\n",
    "        seq =  [dictionary.token2id[word]+1 for word in lst]\n",
    "    else:\n",
    "        new_lst = list(set(lst))\n",
    "        if len(new_lst) <= max_seq_len:\n",
    "            seq = [dictionary.token2id[word]+1 for word in new_lst]\n",
    "        else:\n",
    "            seq = [dictionary.token2id[new_lst[i]]+1 for i in sorted(random.sample(range(len(new_lst)), max_seq_len))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf89c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['seq1'] = data.apply(lambda row: text_to_seq(row.lst1), axis = 1)\n",
    "data['seq2'] = data.apply(lambda row: text_to_seq(row.lst2), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79e55882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>lst1</th>\n",
       "      <th>lst2</th>\n",
       "      <th>seq1</th>\n",
       "      <th>seq2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191740</th>\n",
       "      <td>291166</td>\n",
       "      <td>291167</td>\n",
       "      <td>Do we really need an election commission in In...</td>\n",
       "      <td>Is election commission of India poorly framed?</td>\n",
       "      <td>0</td>\n",
       "      <td>[really, need, election, commission, India]</td>\n",
       "      <td>[election, commission, India, poorly, framed]</td>\n",
       "      <td>[5, 4, 3, 2, 1]</td>\n",
       "      <td>[3, 2, 1, 9829, 26338]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89303</th>\n",
       "      <td>150062</td>\n",
       "      <td>150063</td>\n",
       "      <td>What is the right way to say \"you're welcome\" ...</td>\n",
       "      <td>What is the right way to say \"live free\" in It...</td>\n",
       "      <td>0</td>\n",
       "      <td>[right, way, say, welcome, italian]</td>\n",
       "      <td>[right, way, say, live, free, italian]</td>\n",
       "      <td>[7, 9, 8, 10, 6]</td>\n",
       "      <td>[7, 9, 8, 262, 528, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124380</th>\n",
       "      <td>200861</td>\n",
       "      <td>200862</td>\n",
       "      <td>How could Russia hack US election?</td>\n",
       "      <td>Can the US redo the 2016 Presidential election...</td>\n",
       "      <td>0</td>\n",
       "      <td>[could, russia, hack, america, election]</td>\n",
       "      <td>[america, redo, 2016, presidential, election, ...</td>\n",
       "      <td>[12, 14, 13, 11, 3]</td>\n",
       "      <td>[11, 7538, 737, 3414, 3, 10696, 14, 4225, 622]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199481</th>\n",
       "      <td>300983</td>\n",
       "      <td>300984</td>\n",
       "      <td>How much initial capital investment would I re...</td>\n",
       "      <td>How much initial capital investment would I re...</td>\n",
       "      <td>1</td>\n",
       "      <td>[much, initial, capital, investment, would, re...</td>\n",
       "      <td>[much, initial, capital, investment, would, re...</td>\n",
       "      <td>[20, 18, 17, 19, 25, 21, 23, 22, 15, 16, 24]</td>\n",
       "      <td>[20, 18, 17, 19, 25, 21, 23, 15, 16, 24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190458</th>\n",
       "      <td>289543</td>\n",
       "      <td>289544</td>\n",
       "      <td>What exactly is the Arab Spring?</td>\n",
       "      <td>What is Arab Spring?</td>\n",
       "      <td>1</td>\n",
       "      <td>[exactly, arab, spring]</td>\n",
       "      <td>[arab, spring]</td>\n",
       "      <td>[27, 26, 28]</td>\n",
       "      <td>[26, 28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328666</th>\n",
       "      <td>455240</td>\n",
       "      <td>455241</td>\n",
       "      <td>What is the most touching real life love story...</td>\n",
       "      <td>What are some examples of patents simple enoug...</td>\n",
       "      <td>0</td>\n",
       "      <td>[touching, real, life, love, story, come, across]</td>\n",
       "      <td>[examples, patents, simple, enough, beginner, ...</td>\n",
       "      <td>[539, 121, 119, 1221, 898, 572, 3560]</td>\n",
       "      <td>[117, 15871, 1311, 1851, 2140, 1266]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384803</th>\n",
       "      <td>45489</td>\n",
       "      <td>125363</td>\n",
       "      <td>Which is the best laptop for gaming under 60k ...</td>\n",
       "      <td>Which is best gaming laptop under 60000?</td>\n",
       "      <td>1</td>\n",
       "      <td>[best, laptop, gaming, 60000, inr]</td>\n",
       "      <td>[best, gaming, laptop, 60000]</td>\n",
       "      <td>[172, 2112, 4945, 7230, 467]</td>\n",
       "      <td>[172, 4945, 2112, 7230]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134515</th>\n",
       "      <td>214973</td>\n",
       "      <td>214974</td>\n",
       "      <td>What are potato bugs and are they harmful to h...</td>\n",
       "      <td>How harmful could it be to have bugs in the re...</td>\n",
       "      <td>0</td>\n",
       "      <td>[potato, bugs, harmful, humans]</td>\n",
       "      <td>[harmful, could, bugs, refrigerator]</td>\n",
       "      <td>[12249, 9165, 1292, 1253]</td>\n",
       "      <td>[1292, 12, 9165, 7467]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382867</th>\n",
       "      <td>514761</td>\n",
       "      <td>514762</td>\n",
       "      <td>When were cotton fibres first used?</td>\n",
       "      <td>Where was cotton first used?</td>\n",
       "      <td>1</td>\n",
       "      <td>[cotton, fibres, first, used]</td>\n",
       "      <td>[cotton, first, used]</td>\n",
       "      <td>[1862, 36305, 321, 365]</td>\n",
       "      <td>[1862, 321, 365]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79939</th>\n",
       "      <td>136010</td>\n",
       "      <td>136011</td>\n",
       "      <td>How is the word \"indomitable\" used in a sentence?</td>\n",
       "      <td>What are career opportunities after entering i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[word, indomitable, used, sentence]</td>\n",
       "      <td>[career, opportunities, entering, fmcg, sales,...</td>\n",
       "      <td>[1493, 36306, 365, 2547]</td>\n",
       "      <td>[394, 4572, 8292, 15250, 1204, 1263, 24654, 10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99958 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid1    qid2                                          question1  \\\n",
       "id                                                                          \n",
       "191740  291166  291167  Do we really need an election commission in In...   \n",
       "89303   150062  150063  What is the right way to say \"you're welcome\" ...   \n",
       "124380  200861  200862                 How could Russia hack US election?   \n",
       "199481  300983  300984  How much initial capital investment would I re...   \n",
       "190458  289543  289544                   What exactly is the Arab Spring?   \n",
       "...        ...     ...                                                ...   \n",
       "328666  455240  455241  What is the most touching real life love story...   \n",
       "384803   45489  125363  Which is the best laptop for gaming under 60k ...   \n",
       "134515  214973  214974  What are potato bugs and are they harmful to h...   \n",
       "382867  514761  514762                When were cotton fibres first used?   \n",
       "79939   136010  136011  How is the word \"indomitable\" used in a sentence?   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "id                                                                        \n",
       "191740     Is election commission of India poorly framed?             0   \n",
       "89303   What is the right way to say \"live free\" in It...             0   \n",
       "124380  Can the US redo the 2016 Presidential election...             0   \n",
       "199481  How much initial capital investment would I re...             1   \n",
       "190458                               What is Arab Spring?             1   \n",
       "...                                                   ...           ...   \n",
       "328666  What are some examples of patents simple enoug...             0   \n",
       "384803           Which is best gaming laptop under 60000?             1   \n",
       "134515  How harmful could it be to have bugs in the re...             0   \n",
       "382867                       Where was cotton first used?             1   \n",
       "79939   What are career opportunities after entering i...             0   \n",
       "\n",
       "                                                     lst1  \\\n",
       "id                                                          \n",
       "191740        [really, need, election, commission, India]   \n",
       "89303                 [right, way, say, welcome, italian]   \n",
       "124380           [could, russia, hack, america, election]   \n",
       "199481  [much, initial, capital, investment, would, re...   \n",
       "190458                            [exactly, arab, spring]   \n",
       "...                                                   ...   \n",
       "328666  [touching, real, life, love, story, come, across]   \n",
       "384803                 [best, laptop, gaming, 60000, inr]   \n",
       "134515                    [potato, bugs, harmful, humans]   \n",
       "382867                      [cotton, fibres, first, used]   \n",
       "79939                 [word, indomitable, used, sentence]   \n",
       "\n",
       "                                                     lst2  \\\n",
       "id                                                          \n",
       "191740      [election, commission, India, poorly, framed]   \n",
       "89303              [right, way, say, live, free, italian]   \n",
       "124380  [america, redo, 2016, presidential, election, ...   \n",
       "199481  [much, initial, capital, investment, would, re...   \n",
       "190458                                     [arab, spring]   \n",
       "...                                                   ...   \n",
       "328666  [examples, patents, simple, enough, beginner, ...   \n",
       "384803                      [best, gaming, laptop, 60000]   \n",
       "134515               [harmful, could, bugs, refrigerator]   \n",
       "382867                              [cotton, first, used]   \n",
       "79939   [career, opportunities, entering, fmcg, sales,...   \n",
       "\n",
       "                                                seq1  \\\n",
       "id                                                     \n",
       "191740                               [5, 4, 3, 2, 1]   \n",
       "89303                               [7, 9, 8, 10, 6]   \n",
       "124380                           [12, 14, 13, 11, 3]   \n",
       "199481  [20, 18, 17, 19, 25, 21, 23, 22, 15, 16, 24]   \n",
       "190458                                  [27, 26, 28]   \n",
       "...                                              ...   \n",
       "328666         [539, 121, 119, 1221, 898, 572, 3560]   \n",
       "384803                  [172, 2112, 4945, 7230, 467]   \n",
       "134515                     [12249, 9165, 1292, 1253]   \n",
       "382867                       [1862, 36305, 321, 365]   \n",
       "79939                       [1493, 36306, 365, 2547]   \n",
       "\n",
       "                                                     seq2  \n",
       "id                                                         \n",
       "191740                             [3, 2, 1, 9829, 26338]  \n",
       "89303                              [7, 9, 8, 262, 528, 6]  \n",
       "124380     [11, 7538, 737, 3414, 3, 10696, 14, 4225, 622]  \n",
       "199481           [20, 18, 17, 19, 25, 21, 23, 15, 16, 24]  \n",
       "190458                                           [26, 28]  \n",
       "...                                                   ...  \n",
       "328666               [117, 15871, 1311, 1851, 2140, 1266]  \n",
       "384803                            [172, 4945, 2112, 7230]  \n",
       "134515                             [1292, 12, 9165, 7467]  \n",
       "382867                                   [1862, 321, 365]  \n",
       "79939   [394, 4572, 8292, 15250, 1204, 1263, 24654, 10...  \n",
       "\n",
       "[99958 rows x 9 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e7b16",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a2e217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e52df09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(data[[\"seq1\", \"seq2\"]], data[\"is_duplicate\"], test_size=0.2, random_state=42, stratify=data[\"is_duplicate\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94e44706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq1</th>\n",
       "      <th>seq2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153275</th>\n",
       "      <td>[108, 910, 1413]</td>\n",
       "      <td>[108, 910]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114523</th>\n",
       "      <td>[2683, 2171, 9157]</td>\n",
       "      <td>[186, 18050, 522]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262353</th>\n",
       "      <td>[3876, 6143, 670, 671]</td>\n",
       "      <td>[3876, 201]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154306</th>\n",
       "      <td>[172, 917, 6747, 3939, 4304]</td>\n",
       "      <td>[172, 917, 6747, 3939, 4304]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167944</th>\n",
       "      <td>[1182, 208, 1937, 2451, 35037, 458, 3941, 3503...</td>\n",
       "      <td>[45205, 208, 2058, 20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338657</th>\n",
       "      <td>[80, 1773, 3246, 1382]</td>\n",
       "      <td>[3246, 152, 80]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332153</th>\n",
       "      <td>[1422, 1797, 6284, 2313, 13980]</td>\n",
       "      <td>[25, 2704, 8866, 15231, 2313, 13980, 1422, 267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132674</th>\n",
       "      <td>[478, 208, 24060]</td>\n",
       "      <td>[1111, 1528, 1529, 311]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357337</th>\n",
       "      <td>[64, 963, 7092, 541]</td>\n",
       "      <td>[963, 58, 380, 262, 541]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235002</th>\n",
       "      <td>[40, 309, 165, 424, 1]</td>\n",
       "      <td>[309, 165, 424]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79966 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     seq1  \\\n",
       "id                                                          \n",
       "153275                                   [108, 910, 1413]   \n",
       "114523                                 [2683, 2171, 9157]   \n",
       "262353                             [3876, 6143, 670, 671]   \n",
       "154306                       [172, 917, 6747, 3939, 4304]   \n",
       "167944  [1182, 208, 1937, 2451, 35037, 458, 3941, 3503...   \n",
       "...                                                   ...   \n",
       "338657                             [80, 1773, 3246, 1382]   \n",
       "332153                    [1422, 1797, 6284, 2313, 13980]   \n",
       "132674                                  [478, 208, 24060]   \n",
       "357337                               [64, 963, 7092, 541]   \n",
       "235002                             [40, 309, 165, 424, 1]   \n",
       "\n",
       "                                                     seq2  \n",
       "id                                                         \n",
       "153275                                         [108, 910]  \n",
       "114523                                  [186, 18050, 522]  \n",
       "262353                                        [3876, 201]  \n",
       "154306                       [172, 917, 6747, 3939, 4304]  \n",
       "167944                             [45205, 208, 2058, 20]  \n",
       "...                                                   ...  \n",
       "338657                                    [3246, 152, 80]  \n",
       "332153  [25, 2704, 8866, 15231, 2313, 13980, 1422, 267...  \n",
       "132674                            [1111, 1528, 1529, 311]  \n",
       "357337                           [963, 58, 380, 262, 541]  \n",
       "235002                                    [309, 165, 424]  \n",
       "\n",
       "[79966 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b23c3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "153275    0\n",
       "114523    0\n",
       "262353    1\n",
       "154306    1\n",
       "167944    0\n",
       "         ..\n",
       "338657    0\n",
       "332153    0\n",
       "132674    0\n",
       "357337    1\n",
       "235002    1\n",
       "Name: is_duplicate, Length: 79966, dtype: int32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5f958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9bb6eef",
   "metadata": {},
   "source": [
    "# Import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5d28c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_FILE = 'glove.6B.100d.txt'\n",
    "embed_dict = {}\n",
    "for o in open(EMBEDDING_FILE, encoding='utf-8'):\n",
    "    o_lst = o.split(\" \")\n",
    "    embed_dict[o_lst[0]] = np.asarray(o_lst[1:], dtype='float32')[:100]\n",
    "embed_dict['the'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6851efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400001"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86b3a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dictionary.token2id) + 1\n",
    "substi_vector = np.asarray([0 for n in range(100)], dtype = 'float32')\n",
    "      \n",
    "# Adding again 1 because of reserved 0 index\n",
    "subs_num = 1\n",
    "embedding_matrix = np.zeros((vocab_size,100))\n",
    "ps = PorterStemmer()\n",
    "for word, idx in dictionary.token2id.items():\n",
    "    if word in embed_dict.keys():\n",
    "        embedding_matrix[idx+1] = embed_dict[word]\n",
    "    else:\n",
    "        mod_word = ps.stem(word)\n",
    "        if mod_word in embed_dict.keys():\n",
    "            embedding_matrix[idx+1] = embed_dict[mod_word]\n",
    "        else:\n",
    "            embedding_matrix[idx+1] = substi_vector\n",
    "            subs_num += 1\n",
    "        \n",
    "embedding_matrix[0] = substi_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "033a7ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7701"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba231bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45865, 100), dtype('float64'))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding_matrix.shape, embedding_matrix.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bace89f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c727b0d",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997b3f3",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff302098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c79ddcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([0]),\n",
       "  tensor([1]),\n",
       "  tensor([2]),\n",
       "  tensor([3]),\n",
       "  tensor([4]),\n",
       "  tensor([5]),\n",
       "  tensor([6]),\n",
       "  tensor([7]),\n",
       "  tensor([8]),\n",
       "  tensor([9])],\n",
       " [tensor([10]),\n",
       "  tensor([11]),\n",
       "  tensor([12]),\n",
       "  tensor([13]),\n",
       "  tensor([14]),\n",
       "  tensor([15]),\n",
       "  tensor([16]),\n",
       "  tensor([17]),\n",
       "  tensor([18]),\n",
       "  tensor([19])],\n",
       " [tensor([0]),\n",
       "  tensor([1]),\n",
       "  tensor([0]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([1]),\n",
       "  tensor([0]),\n",
       "  tensor([1]),\n",
       "  tensor([0]),\n",
       "  tensor([0])])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader1_test = [torch.tensor([n], dtype = torch.long) for n in range(10)]\n",
    "dataloader2_test = [torch.tensor([n], dtype = torch.long) for n in range(10, 20)]\n",
    "dataloader3_test = [torch.tensor([random.choice([0,1])], dtype = torch.long) for n in range(20, 30)]\n",
    "(dataloader1_test, dataloader2_test, dataloader3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6de135bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test_train = MyDataset(dataloader1_test, dataloader2_test, dataloader3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3ef107bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_test_train.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e47b00c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5]), tensor([15]), tensor([1]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_test_train.__getitem__(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "51156451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    r\"\"\"Dataset wrapping tensors.\n",
    "\n",
    "    Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "\n",
    "    Args:\n",
    "        *tensors (Tensor): tensors that have the same size of the first dimension.\n",
    "    \"\"\"\n",
    "    #tensors: Tuple[Tensor, ...]\n",
    "\n",
    "    def __init__(self, lst1, lst2, lst3) -> None:\n",
    "        assert len(lst1) == len(lst2) and len(lst1) == len(lst3), \"Size mismatch between tensors\"\n",
    "        self.lst1 = lst1\n",
    "        self.lst2 = lst2\n",
    "        self.lst3 = lst3\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple([self.lst1[index], self.lst2[index], self.lst3[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1cfdc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f53986d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_dataloader = DataLoader(\n",
    "            dataloader_test_train,  # The training samples.\n",
    "            shuffle=True, # Select batches randomly\n",
    "            batch_size = 4, # Trains with this batch size.\n",
    "            num_workers = 0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f2d45e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[6],\n",
      "        [8],\n",
      "        [0],\n",
      "        [7]]), tensor([[16],\n",
      "        [18],\n",
      "        [10],\n",
      "        [17]]), tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1]])]\n",
      "[tensor([[2],\n",
      "        [5],\n",
      "        [4],\n",
      "        [9]]), tensor([[12],\n",
      "        [15],\n",
      "        [14],\n",
      "        [19]]), tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]])]\n",
      "[tensor([[1],\n",
      "        [3]]), tensor([[11],\n",
      "        [13]]), tensor([[1],\n",
      "        [1]])]\n"
     ]
    }
   ],
   "source": [
    "for batch in test_train_dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913c55a",
   "metadata": {},
   "source": [
    "#### New way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6f2a49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79966, 79966, 79966)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1 = [torch.tensor(np.asarray([lst], dtype='long'), dtype=torch.long) for lst in X_train['seq1'].values]\n",
    "X_train_2 = [torch.tensor(np.asarray([lst], dtype='long'), dtype=torch.long) for lst in X_train['seq2'].values]\n",
    "\n",
    "Y_train =  [torch.tensor(np.asarray([lst], dtype='float32'), dtype=torch.float32) for lst in y_train.values]\n",
    "\n",
    "(len(X_train_1), len(X_train_2), len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7416b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "98926426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79968"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = [tuple([X_train_1[n], X_train_2[n], Y_train[n]]) for n in range(len(X_train_1))]\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3b2c0cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  282,   155,  4880, 14420]]),\n",
       " tensor([[4880, 6540, 5423,  282,  155, 4880]]),\n",
       " tensor([1.]))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3855c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae5dea48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m shuffle(\u001b[43mtrain_dataset\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = shuffle(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "213c6609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79968"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2dd3a7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([  11,  318,  395, 4892]),\n",
       "  tensor([  461, 19277,   395,  4892]),\n",
       "  tensor(1.)),\n",
       " (tensor([  65,   97, 5904,  647, 3199]),\n",
       "  tensor([ 647, 5291,   97, 5160]),\n",
       "  tensor(1.)),\n",
       " (tensor([  384,   385,   591,   733,  5018, 18042,   145]),\n",
       "  tensor([  384,   385,   591,   733,  5018, 18042]),\n",
       "  tensor(0.)),\n",
       " (tensor([475,  97,  57]), tensor([205, 475,  97]), tensor(1.)),\n",
       " (tensor([10975,   318, 35486]),\n",
       "  tensor([ 745, 1197,   42,   11,   26,  728, 6860]),\n",
       "  tensor(0.)),\n",
       " (tensor([  68, 4126]), tensor([  68, 4126,   26]), tensor(0.)),\n",
       " (tensor([18151,  2931]), tensor([18151,  2931,  8242]), tensor(0.)),\n",
       " (tensor([154, 155, 172, 414, 750]),\n",
       "  tensor([ 154,  155, 1008,   34,  172]),\n",
       "  tensor(1.)),\n",
       " (tensor([2316,  686]), tensor([2316,  686]), tensor(1.)),\n",
       " (tensor([1366, 1362,  874,  547, 1075, 1363, 1364,  874,  547, 1365]),\n",
       "  tensor([2532,  999, 6974,  874,  547, 1365, 1362]),\n",
       "  tensor(0.))]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a00112",
   "metadata": {},
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8becd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_1 = [torch.tensor(np.asarray([lst], dtype='long'), dtype=torch.long) for lst in X_validation['seq1'].values]\n",
    "X_valid_2 = [torch.tensor(np.asarray([lst], dtype='long'), dtype=torch.long) for lst in X_validation['seq2'].values]\n",
    "\n",
    "Y_valid =  [torch.tensor(np.asarray([lst], dtype='float32'), dtype=torch.float32) for lst in y_validation.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d0097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76fd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cde4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d26ad244",
   "metadata": {},
   "source": [
    "# BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3b4a9",
   "metadata": {},
   "source": [
    "##### <span style=\"color:blue\">Test</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3bca9d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sample = train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9ac75",
   "metadata": {},
   "source": [
    "Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a57ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding = nn.Embedding(embedding_dim, 100)\n",
    "test_embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "beac1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds1 = test_embedding(batch_sample[0])\n",
    "embeds2 = test_embedding(batch_sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3996fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 100]), torch.Size([1, 6, 100]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embeds1.shape, embeds2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e5bb0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds1 = embeds1.view(1,embeds1.size(0), embeds1.size(1))\n",
    "embeds2 = embeds2.view(1,embeds2.size(0), embeds2.size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b355c",
   "metadata": {},
   "source": [
    "Bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e303db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm1 = nn.LSTM(input_size = 100, hidden_size = hidden_dim, batch_first=True, bidirectional = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6f730d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out1, hc1 = test_lstm1(embeds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "53dc2e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out2,_ = test_lstm1(embeds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9d2e30dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 100]), torch.Size([1, 6, 100]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lstm_out1.shape, lstm_out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0bb5d379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0073,  0.1217, -0.0217,  0.1182, -0.0436,  0.0426,  0.0137,\n",
       "           0.0694,  0.0949,  0.1114, -0.1540,  0.1062, -0.0846,  0.1304,\n",
       "          -0.1763,  0.1521, -0.1003, -0.0505, -0.0207,  0.1399, -0.1157,\n",
       "          -0.0611, -0.2948, -0.1102,  0.0740,  0.0203, -0.1087,  0.0282,\n",
       "          -0.0162,  0.0891,  0.0488, -0.0193,  0.0528,  0.0944,  0.1920,\n",
       "          -0.0274, -0.0781, -0.0602, -0.0824, -0.1415,  0.1435,  0.0725,\n",
       "           0.1375,  0.0440,  0.0505, -0.0192,  0.1977,  0.1542, -0.0134,\n",
       "           0.1737, -0.1448, -0.2157, -0.0889,  0.1663, -0.2169,  0.0549,\n",
       "           0.2533,  0.0240, -0.1593, -0.0357, -0.0649,  0.0618,  0.0010,\n",
       "           0.0644,  0.1356, -0.1475,  0.1889, -0.2118, -0.0790,  0.0768,\n",
       "           0.2867,  0.0609,  0.2869,  0.0344, -0.1935,  0.1714,  0.0324,\n",
       "           0.1643, -0.2180, -0.0319, -0.0578,  0.2311,  0.0850,  0.0341,\n",
       "           0.0640, -0.2028, -0.0404, -0.0095,  0.1074, -0.3042, -0.0905,\n",
       "          -0.0535, -0.1194, -0.1921, -0.0334, -0.0723,  0.2104,  0.0341,\n",
       "           0.1238,  0.1823],\n",
       "         [-0.0784,  0.1630, -0.0721,  0.0579, -0.1960, -0.0581, -0.0023,\n",
       "           0.1552,  0.1125,  0.1967, -0.1903,  0.2062, -0.2184,  0.1949,\n",
       "          -0.1830,  0.2332,  0.0225, -0.1392, -0.1315,  0.0671, -0.1075,\n",
       "          -0.1271, -0.4402, -0.1303,  0.0326,  0.0735, -0.0114,  0.0808,\n",
       "           0.0965,  0.0395,  0.1258, -0.0593,  0.0862,  0.0344,  0.0405,\n",
       "          -0.0718, -0.1106, -0.0283, -0.1125,  0.0341,  0.1881,  0.1072,\n",
       "           0.1803, -0.0307,  0.0851,  0.1683,  0.2151,  0.1580, -0.0853,\n",
       "           0.1762, -0.2102, -0.0762, -0.0809,  0.1337, -0.1933,  0.1860,\n",
       "           0.1687,  0.0106, -0.0915, -0.0328, -0.0173, -0.1639,  0.0513,\n",
       "           0.0578,  0.0404, -0.2638,  0.2611, -0.1481, -0.0928,  0.0596,\n",
       "           0.2346,  0.0870,  0.2002,  0.1628, -0.1786,  0.1412,  0.0964,\n",
       "           0.1663, -0.1672, -0.0311, -0.0879,  0.2581,  0.1035,  0.0553,\n",
       "          -0.0355, -0.0832, -0.0016, -0.0038,  0.1344, -0.2174, -0.0786,\n",
       "          -0.0718, -0.1628, -0.2214, -0.0116,  0.0295,  0.1007,  0.1099,\n",
       "           0.0375,  0.2281],\n",
       "         [-0.0856,  0.2446, -0.0289,  0.0501, -0.0498, -0.1525, -0.1531,\n",
       "           0.0853,  0.0433,  0.1169, -0.2075,  0.1869, -0.0444,  0.2452,\n",
       "          -0.1758,  0.1068,  0.0891, -0.1206,  0.0877, -0.0809,  0.0354,\n",
       "          -0.0078, -0.1977,  0.0234,  0.0433,  0.0988,  0.0191,  0.0904,\n",
       "          -0.0861,  0.1267, -0.0941, -0.0484,  0.1014,  0.1025,  0.0716,\n",
       "          -0.0648, -0.2251,  0.0028, -0.0908, -0.0529,  0.1048,  0.1444,\n",
       "           0.2355,  0.0305, -0.0213,  0.2156,  0.3240,  0.1678, -0.0497,\n",
       "           0.0155, -0.1600,  0.0978, -0.1038,  0.0024, -0.1235,  0.0960,\n",
       "          -0.0484,  0.0908, -0.2008, -0.0208,  0.0692,  0.0914,  0.0658,\n",
       "          -0.0171, -0.0947, -0.3053,  0.1912, -0.2395,  0.0285, -0.0779,\n",
       "          -0.0283,  0.0036, -0.0371,  0.0984,  0.0102,  0.0545,  0.3026,\n",
       "           0.1677, -0.0494,  0.0473, -0.0553,  0.1152,  0.0865, -0.0362,\n",
       "          -0.1529,  0.0340,  0.0136, -0.0153,  0.0069,  0.0076, -0.0393,\n",
       "           0.0532,  0.0387, -0.1783, -0.0734, -0.1518, -0.0318, -0.0295,\n",
       "          -0.1276,  0.3383],\n",
       "         [-0.0306,  0.1480, -0.2445,  0.0267,  0.0208, -0.1850, -0.0981,\n",
       "           0.1398, -0.1411,  0.0620,  0.0233,  0.1925, -0.1007,  0.2788,\n",
       "           0.0654, -0.0724, -0.0561, -0.1529,  0.0128, -0.0602,  0.2552,\n",
       "           0.0079, -0.1903, -0.1445, -0.0123,  0.0041, -0.1440,  0.1949,\n",
       "           0.0252,  0.0752, -0.1384,  0.1050,  0.2613,  0.2094,  0.0180,\n",
       "          -0.0249, -0.0144, -0.2120, -0.1087,  0.0555, -0.0110,  0.0707,\n",
       "           0.1783,  0.0598,  0.0378,  0.3479,  0.4340,  0.1660, -0.0746,\n",
       "          -0.1366, -0.1893,  0.1039, -0.2125, -0.0435, -0.1032,  0.0372,\n",
       "          -0.0723,  0.1108, -0.1048, -0.0554,  0.1670, -0.0263, -0.0726,\n",
       "           0.2168, -0.1549, -0.2631,  0.0657, -0.3441,  0.0709,  0.0039,\n",
       "          -0.1620,  0.0086, -0.1193,  0.0314,  0.1272,  0.0178,  0.2385,\n",
       "           0.0611, -0.1006, -0.2488, -0.1465,  0.1359,  0.0435,  0.0755,\n",
       "          -0.1743,  0.1169,  0.1100, -0.0466,  0.0976, -0.0496,  0.0039,\n",
       "          -0.0077,  0.0653, -0.0468, -0.0556, -0.1612, -0.0796, -0.1457,\n",
       "          -0.0952,  0.2301]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f7ccb2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0306,  0.1480, -0.2445,  0.0267,  0.0208, -0.1850, -0.0981,\n",
       "            0.1398, -0.1411,  0.0620,  0.0233,  0.1925, -0.1007,  0.2788,\n",
       "            0.0654, -0.0724, -0.0561, -0.1529,  0.0128, -0.0602,  0.2552,\n",
       "            0.0079, -0.1903, -0.1445, -0.0123,  0.0041, -0.1440,  0.1949,\n",
       "            0.0252,  0.0752, -0.1384,  0.1050,  0.2613,  0.2094,  0.0180,\n",
       "           -0.0249, -0.0144, -0.2120, -0.1087,  0.0555, -0.0110,  0.0707,\n",
       "            0.1783,  0.0598,  0.0378,  0.3479,  0.4340,  0.1660, -0.0746,\n",
       "           -0.1366]],\n",
       " \n",
       "         [[-0.1448, -0.2157, -0.0889,  0.1663, -0.2169,  0.0549,  0.2533,\n",
       "            0.0240, -0.1593, -0.0357, -0.0649,  0.0618,  0.0010,  0.0644,\n",
       "            0.1356, -0.1475,  0.1889, -0.2118, -0.0790,  0.0768,  0.2867,\n",
       "            0.0609,  0.2869,  0.0344, -0.1935,  0.1714,  0.0324,  0.1643,\n",
       "           -0.2180, -0.0319, -0.0578,  0.2311,  0.0850,  0.0341,  0.0640,\n",
       "           -0.2028, -0.0404, -0.0095,  0.1074, -0.3042, -0.0905, -0.0535,\n",
       "           -0.1194, -0.1921, -0.0334, -0.0723,  0.2104,  0.0341,  0.1238,\n",
       "            0.1823]]], grad_fn=<StackBackward>),\n",
       " tensor([[[-0.0633,  0.4285, -0.6525,  0.1053,  0.0390, -0.3827, -0.3830,\n",
       "            0.2889, -0.2845,  0.1495,  0.0471,  0.3217, -0.2007,  0.5453,\n",
       "            0.1547, -0.1425, -0.0921, -0.2379,  0.0267, -0.0815,  0.3953,\n",
       "            0.0180, -0.4629, -0.3382, -0.0387,  0.0115, -0.2750,  0.3737,\n",
       "            0.0420,  0.2901, -0.2581,  0.1767,  0.3996,  0.4375,  0.0411,\n",
       "           -0.0414, -0.0238, -0.3683, -0.3905,  0.1177, -0.0287,  0.1355,\n",
       "            0.4147,  0.1518,  0.0574,  0.5453,  0.6370,  0.3465, -0.1344,\n",
       "           -0.2369]],\n",
       " \n",
       "         [[-0.2078, -0.5143, -0.2121,  0.2966, -0.6288,  0.1450,  0.6742,\n",
       "            0.0498, -0.2437, -0.0813, -0.1609,  0.1231,  0.0020,  0.1000,\n",
       "            0.3539, -0.2410,  0.4992, -0.3670, -0.1172,  0.1554,  0.4893,\n",
       "            0.1755,  0.3888,  0.0711, -0.3489,  0.4534,  0.0511,  0.3012,\n",
       "           -0.4535, -0.0705, -0.1581,  0.5291,  0.2079,  0.0628,  0.1493,\n",
       "           -0.4199, -0.1008, -0.0168,  0.3199, -0.5358, -0.2486, -0.1270,\n",
       "           -0.2534, -0.6125, -0.0978, -0.2065,  0.4035,  0.0878,  0.2172,\n",
       "            0.6976]]], grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3183a",
   "metadata": {},
   "source": [
    "Matching layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b470d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matchpara1 = nn.Parameter(torch.rand(match_size, hidden_dim))\n",
    "test_matchpara2 = nn.Parameter(torch.rand(match_size, hidden_dim))\n",
    "test_cosSim = nn.CosineSimilarity(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "747d132f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 3, 4])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cat1 = torch.tensor([1,2,3])\n",
    "test_cat2 = torch.tensor([3,4])\n",
    "torch.cat((test_cat1, test_cat2), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0daf8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "143c4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_matching_method(batch1_t, lstm_out2_batch1):\n",
    "    prep_batch1_t = batch1_t.repeat(lstm_out2_batch1.size(0),1)\n",
    "    weight = test_cosSim(prep_batch1_t, lstm_out2_batch1).detach().numpy()\n",
    "    max_weight = np.argmax(weight)\n",
    "    return lstm_out2_batch1[max_weight,:]\n",
    "\n",
    "def test_match_cal(lstm_out1, lstm_out2):\n",
    "    match_out_all = []\n",
    "    for b in range(lstm_out1.size(0)):\n",
    "        match_out_lst = []\n",
    "        lstm_out1_batch1 = lstm_out1[b,:,:]\n",
    "        #lstm_out2_batch1_end = lstm_out2[b, -1, :]\n",
    "        for seq in range(lstm_out1_batch1.size(0)):\n",
    "            batch1_t_first = lstm_out1_batch1[seq,:hidden_dim]\n",
    "            #batch2_t = lstm_out2_batch1_end\n",
    "            batch2_t_first = test_matching_method(batch1_t_first, lstm_out2[b,:,:hidden_dim])\n",
    "            mod_batch1_t_first = torch.mul(test_matchpara1, batch1_t_first)\n",
    "            mod_batch2_t_first = torch.mul(test_matchpara1, batch2_t_first)\n",
    "            match_out_t_first = test_cosSim(mod_batch1_t_first, mod_batch2_t_first)\n",
    "            batch1_t_second = lstm_out1_batch1[seq,hidden_dim:]\n",
    "            batch2_t_second = test_matching_method(batch1_t_second, lstm_out2[b,:,hidden_dim:])\n",
    "            mod_batch1_t_second = torch.mul(test_matchpara2, batch1_t_second)\n",
    "            mod_batch2_t_second = torch.mul(test_matchpara2, batch2_t_second)\n",
    "            match_out_t_second = test_cosSim(mod_batch1_t_second, mod_batch2_t_second)\n",
    "            match_out_t = torch.cat((match_out_t_first, match_out_t_second), dim=0)\n",
    "            match_out_lst.append(match_out_t)\n",
    "        match_out = torch.stack(match_out_lst)\n",
    "        match_out_all.append(match_out)\n",
    "    return torch.stack(match_out_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cd0a1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_match1 = test_match_cal(lstm_out1, lstm_out2)\n",
    "test_match2 = test_match_cal(lstm_out2, lstm_out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b59cf810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 4, 60]), torch.Size([1, 6, 60]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_match1.size(), test_match2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ea1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b88023ab",
   "metadata": {},
   "source": [
    "New lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c1861128",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_new1 = nn.LSTM(input_size = match_size*2, hidden_size = hidden_dim2, batch_first=True, bidirectional = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5e9da2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,(test_lstm_new_out1,_) = test_lstm_new1(test_match1)\n",
    "_,(test_lstm_new_out2,_) = test_lstm_new1(test_match2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "29991d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 20]), torch.Size([2, 1, 20]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_lstm_new_out1.size(), test_lstm_new_out2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4518b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "262d6eb5",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "541c7207",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm_new_out1 = test_lstm_new_out1.view(test_lstm_new_out1.size(0), test_lstm_new_out1.size(2))\n",
    "test_lstm_new_out2 = test_lstm_new_out2.view(test_lstm_new_out2.size(0), test_lstm_new_out2.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f0daf415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.1465e-01, -1.6745e-01,  3.1715e-02,  1.8416e-01, -1.7783e-01,\n",
       "         -2.7314e-01,  2.8250e-01,  4.5750e-02, -2.4371e-01, -2.4981e-02,\n",
       "          3.5792e-02, -1.4749e-01, -3.1761e-01, -2.4772e-01,  4.9829e-01,\n",
       "         -4.7374e-02, -5.3364e-04,  4.8119e-01,  3.1403e-02,  1.5268e-01],\n",
       "        [ 1.4047e-02,  3.5429e-01, -3.0918e-02, -1.1251e-01, -3.2316e-01,\n",
       "         -3.1126e-01, -1.3789e-01, -2.6232e-01,  1.6171e-03,  3.0519e-01,\n",
       "         -6.0726e-01,  4.0246e-01, -1.5949e-01, -2.9613e-01, -3.1445e-02,\n",
       "          1.3803e-01,  9.1508e-02,  2.2612e-01, -8.3417e-02,  2.1938e-01]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lstm_new_out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "19390b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7324, -0.0926,  0.1145,  0.2461, -0.1540, -0.2603,  0.2289,  0.0330,\n",
       "         -0.2470,  0.0690,  0.0489, -0.1343, -0.3406, -0.2146,  0.5267, -0.0347,\n",
       "          0.0228,  0.5530,  0.0456,  0.1882],\n",
       "        [ 0.0426,  0.2714,  0.0720, -0.1700, -0.3775, -0.3128, -0.1702, -0.2518,\n",
       "          0.0155,  0.2878, -0.5411,  0.4220, -0.1490, -0.2301, -0.0282,  0.1775,\n",
       "          0.0708,  0.2244, -0.1130,  0.2346]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lstm_new_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2f86d66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.1465e-01, -1.6745e-01,  3.1715e-02,  1.8416e-01, -1.7783e-01,\n",
       "         -2.7314e-01,  2.8250e-01,  4.5750e-02, -2.4371e-01, -2.4981e-02,\n",
       "          3.5792e-02, -1.4749e-01, -3.1761e-01, -2.4772e-01,  4.9829e-01,\n",
       "         -4.7374e-02, -5.3364e-04,  4.8119e-01,  3.1403e-02,  1.5268e-01,\n",
       "          1.4047e-02,  3.5429e-01, -3.0918e-02, -1.1251e-01, -3.2316e-01,\n",
       "         -3.1126e-01, -1.3789e-01, -2.6232e-01,  1.6171e-03,  3.0519e-01,\n",
       "         -6.0726e-01,  4.0246e-01, -1.5949e-01, -2.9613e-01, -3.1445e-02,\n",
       "          1.3803e-01,  9.1508e-02,  2.2612e-01, -8.3417e-02,  2.1938e-01,\n",
       "         -7.3243e-01, -9.2614e-02,  1.1453e-01,  2.4609e-01, -1.5401e-01,\n",
       "         -2.6027e-01,  2.2886e-01,  3.2998e-02, -2.4700e-01,  6.8988e-02,\n",
       "          4.8853e-02, -1.3435e-01, -3.4062e-01, -2.1463e-01,  5.2670e-01,\n",
       "         -3.4679e-02,  2.2761e-02,  5.5298e-01,  4.5591e-02,  1.8819e-01,\n",
       "          4.2639e-02,  2.7135e-01,  7.1964e-02, -1.7001e-01, -3.7750e-01,\n",
       "         -3.1278e-01, -1.7021e-01, -2.5179e-01,  1.5482e-02,  2.8785e-01,\n",
       "         -5.4111e-01,  4.2199e-01, -1.4896e-01, -2.3007e-01, -2.8246e-02,\n",
       "          1.7754e-01,  7.0809e-02,  2.2440e-01, -1.1303e-01,  2.3458e-01]],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lstm_new_out = torch.cat((test_lstm_new_out1, test_lstm_new_out2), dim = 0).flatten()\n",
    "test_lstm_new_out = test_lstm_new_out.view(1, test_lstm_new_out.size(0))\n",
    "test_lstm_new_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9d08b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hiddenlinear = nn.Linear(4*hidden_dim2, hidden_dim3)\n",
    "test_predlinear = nn.Linear(hidden_dim3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "92c366a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4285e-01,  7.5127e-02,  1.8495e-01,  4.7088e-02, -2.4231e-01,\n",
       "         -1.2013e-01,  1.2017e-01,  1.4376e-01, -2.6500e-01, -1.1666e-01,\n",
       "          6.5637e-02,  2.4466e-01, -1.5559e-01,  6.1745e-02, -6.7375e-02,\n",
       "         -2.2390e-02,  1.3145e-01,  2.2541e-01, -3.6408e-02,  1.7200e-01,\n",
       "          1.2756e-01, -1.4357e-01, -2.1728e-02,  1.3567e-01, -8.1019e-03,\n",
       "          2.1885e-01,  1.8540e-01, -2.0861e-02, -1.9221e-01,  3.4208e-01,\n",
       "          3.3577e-02,  7.2601e-03,  1.2733e-01, -8.4238e-02, -2.8682e-01,\n",
       "         -1.1801e-01,  1.8170e-01, -7.1025e-02, -6.6506e-04,  6.4207e-02,\n",
       "          3.4252e-02, -2.9310e-04, -1.3552e-02,  7.4504e-02,  2.9108e-01,\n",
       "         -1.4040e-01,  1.8030e-02, -1.8633e-01,  1.4907e-01,  4.6020e-02,\n",
       "         -1.3210e-01,  3.2084e-01,  1.4025e-01, -4.5091e-02,  2.0774e-01,\n",
       "         -2.2954e-01, -3.4594e-02, -2.0626e-02,  4.0186e-01, -2.6521e-02,\n",
       "          1.9059e-01, -3.8375e-01,  2.5545e-03, -2.3291e-01,  2.6083e-01,\n",
       "          2.2062e-01, -3.4279e-02, -1.9139e-02, -5.4929e-02,  1.1793e-01,\n",
       "          1.9026e-01,  2.7106e-02, -5.1576e-02,  1.0491e-01,  9.3305e-02,\n",
       "         -2.1634e-01,  9.0170e-02, -1.6846e-01,  1.2668e-01, -1.0802e-01,\n",
       "         -1.7796e-01, -2.0462e-01,  5.3230e-02,  4.5929e-02,  1.4126e-01,\n",
       "          8.0526e-04, -8.0967e-02,  3.6769e-02, -8.7118e-02, -2.0225e-01,\n",
       "         -9.9373e-02, -2.1076e-01, -1.0367e-01, -2.2643e-01,  1.7109e-02,\n",
       "         -2.6846e-03,  1.6198e-01, -2.0504e-01, -1.6563e-01, -1.8844e-02]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_result = test_hiddenlinear(test_lstm_new_out)\n",
    "hidden_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c4790d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0665, -0.1767]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result = test_predlinear(hidden_result)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "da8c7fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5605, 0.4395]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_scores = F.softmax(final_result, dim=1)\n",
    "tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d037ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "730bea3a",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee9be6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2fa69092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45865"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = vocab_size\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "933da984",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 100\n",
    "match_size = 20\n",
    "hidden_dim2 = 100\n",
    "hidden_dim3 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0224e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "52497fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_copy = np.copy(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6b2281f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, match_size, hidden_dim2, hidden_dim3):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(embedding_dim, 100)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix_copy, dtype=torch.float32)) \n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_size = 100, hidden_size = hidden_dim, batch_first = True, bidirectional = True, dropout = 0.1)\n",
    "        \n",
    "        # Matching layer\n",
    "        self.matchpara1 = nn.Parameter(torch.rand(match_size, hidden_dim))\n",
    "        self.matchpara2 = nn.Parameter(torch.rand(match_size, hidden_dim))\n",
    "        self.cosSim = nn.CosineSimilarity(dim=1)\n",
    "        \n",
    "        # Aggregation layer\n",
    "        self.lstm_new = nn.LSTM(input_size = match_size*2, hidden_size = hidden_dim2, batch_first=True, bidirectional = True, dropout = 0.1)\n",
    "        \n",
    "        # Prediction layer\n",
    "        self.hiddenlinear = nn.Linear(4*hidden_dim2, hidden_dim3)\n",
    "        self.predlinear = nn.Linear(hidden_dim3, 2)\n",
    "        \n",
    "    def forward(self, input1, input2):\n",
    "        #print(input1.size())\n",
    "        embeds1 = self.embedding(input1)\n",
    "        #print(embeds1.size())\n",
    "        lstm_out1, _ = self.lstm(embeds1)\n",
    "        embeds2 = self.embedding(input2)\n",
    "        lstm_out2, _ = self.lstm(embeds2)\n",
    "        \n",
    "        # matching\n",
    "        match_1 = self.match_cal(lstm_out1, lstm_out2)\n",
    "        match_2 = self.match_cal(lstm_out2, lstm_out1)\n",
    "        \n",
    "        # aggregating\n",
    "        _,(lstm_new_out1,_) = self.lstm_new(match_1)\n",
    "        _,(lstm_new_out2,_) = self.lstm_new(match_2)\n",
    "        \n",
    "        # predicting\n",
    "        lstm_new_out1 = lstm_new_out1.view(lstm_new_out1.size(0), lstm_new_out1.size(2))\n",
    "        lstm_new_out2 = lstm_new_out2.view(lstm_new_out2.size(0), lstm_new_out2.size(2))\n",
    "        concat_out = torch.cat((lstm_new_out1, lstm_new_out2), dim = 0).flatten()\n",
    "        concat_out = concat_out.view(1, concat_out.size(0))\n",
    "        hidden_result = self.hiddenlinear(concat_out)\n",
    "        final_result = self.predlinear(hidden_result)\n",
    "        tag_scores = F.softmax(final_result, dim=1)\n",
    "        \n",
    "        return tag_scores\n",
    "    \n",
    "    def match_cal(self, lstm_out1, lstm_out2):\n",
    "        match_out_all = []\n",
    "        for b in range(lstm_out1.size(0)):\n",
    "            match_out_lst = []\n",
    "            lstm_out1_batch1 = lstm_out1[b,:,:]\n",
    "            #lstm_out2_batch1_end = lstm_out2[b, -1, :]\n",
    "            for seq in range(lstm_out1_batch1.size(0)):\n",
    "                batch1_t_first = lstm_out1_batch1[seq,:hidden_dim]\n",
    "                #batch2_t = lstm_out2_batch1_end\n",
    "                batch2_t_first = self.matching_method(batch1_t_first, lstm_out2[b,:,:hidden_dim])\n",
    "                mod_batch1_t_first = torch.mul(self.matchpara1, batch1_t_first)\n",
    "                mod_batch2_t_first = torch.mul(self.matchpara1, batch2_t_first)\n",
    "                match_out_t_first = self.cosSim(mod_batch1_t_first, mod_batch2_t_first)\n",
    "                \n",
    "                batch1_t_second = lstm_out1_batch1[seq,hidden_dim:]\n",
    "                batch2_t_second = self.matching_method(batch1_t_second, lstm_out2[b,:,hidden_dim:])\n",
    "                mod_batch1_t_second = torch.mul(self.matchpara2, batch1_t_second)\n",
    "                mod_batch2_t_second = torch.mul(self.matchpara2, batch2_t_second)\n",
    "                match_out_t_second = self.cosSim(mod_batch1_t_second, mod_batch2_t_second)\n",
    "                \n",
    "                match_out_t = torch.cat((match_out_t_first, match_out_t_second), dim=0)\n",
    "                match_out_lst.append(match_out_t)\n",
    "            match_out = torch.stack(match_out_lst)\n",
    "            match_out_all.append(match_out)\n",
    "        return torch.stack(match_out_all)\n",
    "    \n",
    "    def matching_method(self, batch1_t, lstm_out2_batch1):\n",
    "        prep_batch1_t = batch1_t.repeat(lstm_out2_batch1.size(0),1)\n",
    "        weight = self.cosSim(prep_batch1_t, lstm_out2_batch1).detach().numpy()\n",
    "        max_weight = np.argmax(weight)\n",
    "        return lstm_out2_batch1[max_weight,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a877a",
   "metadata": {},
   "source": [
    "### Train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05d24ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a868e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "85142d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMmodel = LSTMTagger(embedding_dim, hidden_dim, match_size, hidden_dim2, hidden_dim3)\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "optimizer = optim.Adam(LSTMmodel.parameters(), lr=(1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "03db36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97246e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2499.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_batch = np.ceil(len(X_train_1)/Batch_size)\n",
    "num_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "33b0d409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_valid_batch = np.ceil(len(X_valid_1) / Batch_size)\n",
    "num_valid_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a149f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbe7b5c4",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">Test</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3b8e16dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f0dbd3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "052a08bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.2177e-08])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_scores[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3516806b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_scores[:,-1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "74cfbc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(tag_scores[:,-1], Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d8ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1115e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3bc6c2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 \t loss=0.010430\n",
      "Batch 21 \t loss=0.216342\n",
      "Batch 41 \t loss=0.420842\n",
      "Batch 61 \t loss=0.627156\n",
      "Batch 81 \t loss=0.834446\n",
      "Batch 101 \t loss=1.042223\n",
      "Batch 121 \t loss=1.248537\n",
      "Batch 141 \t loss=1.455592\n",
      "Batch 161 \t loss=1.656785\n",
      "Batch 181 \t loss=1.859360\n",
      "Batch 201 \t loss=2.069614\n",
      "Batch 221 \t loss=2.271307\n",
      "Batch 241 \t loss=2.476224\n",
      "Batch 261 \t loss=2.678895\n",
      "Batch 281 \t loss=2.878839\n",
      "Batch 301 \t loss=3.083795\n",
      "Batch 321 \t loss=3.290498\n",
      "Batch 341 \t loss=3.494165\n",
      "Batch 361 \t loss=3.700553\n",
      "Batch 381 \t loss=3.902904\n",
      "Batch 401 \t loss=4.103524\n",
      "Batch 421 \t loss=4.306669\n",
      "Batch 441 \t loss=4.505113\n",
      "Batch 461 \t loss=4.706044\n",
      "Batch 481 \t loss=4.908590\n",
      "Batch 501 \t loss=5.109792\n",
      "Batch 521 \t loss=5.313713\n",
      "Batch 541 \t loss=5.512243\n",
      "Batch 561 \t loss=5.712338\n",
      "Batch 581 \t loss=5.913498\n",
      "Batch 601 \t loss=6.117137\n",
      "Batch 621 \t loss=6.317957\n",
      "Batch 641 \t loss=6.519524\n",
      "Batch 661 \t loss=6.718201\n",
      "Batch 681 \t loss=6.921604\n",
      "Batch 701 \t loss=7.125202\n",
      "Batch 721 \t loss=7.326595\n",
      "Batch 741 \t loss=7.529015\n",
      "Batch 761 \t loss=7.726392\n",
      "Batch 781 \t loss=7.924757\n",
      "Batch 801 \t loss=8.124961\n",
      "Batch 821 \t loss=8.328227\n",
      "Batch 841 \t loss=8.527347\n",
      "Batch 861 \t loss=8.726933\n",
      "Batch 881 \t loss=8.925848\n",
      "Batch 901 \t loss=9.121321\n",
      "Batch 921 \t loss=9.316905\n",
      "Batch 941 \t loss=9.515370\n",
      "Batch 961 \t loss=9.716613\n",
      "Batch 981 \t loss=9.915736\n",
      "Batch 1001 \t loss=10.115990\n",
      "Batch 1021 \t loss=10.315534\n",
      "Batch 1041 \t loss=10.515830\n",
      "Batch 1061 \t loss=10.711849\n",
      "Batch 1081 \t loss=10.908353\n",
      "Batch 1101 \t loss=11.104707\n",
      "Batch 1121 \t loss=11.304328\n",
      "Batch 1141 \t loss=11.501868\n",
      "Batch 1161 \t loss=11.703162\n",
      "Batch 1181 \t loss=11.898642\n",
      "Batch 1201 \t loss=12.102007\n",
      "Batch 1221 \t loss=12.297106\n",
      "Batch 1241 \t loss=12.496622\n",
      "Batch 1261 \t loss=12.692535\n",
      "Batch 1281 \t loss=12.887516\n",
      "Batch 1301 \t loss=13.086532\n",
      "Batch 1321 \t loss=13.285755\n",
      "Batch 1341 \t loss=13.481012\n",
      "Batch 1361 \t loss=13.677946\n",
      "Batch 1381 \t loss=13.869721\n",
      "Batch 1401 \t loss=14.067662\n",
      "Batch 1421 \t loss=14.266590\n",
      "Batch 1441 \t loss=14.465045\n",
      "Batch 1461 \t loss=14.656866\n",
      "Batch 1481 \t loss=14.850352\n",
      "Batch 1501 \t loss=15.045806\n",
      "Batch 1521 \t loss=15.237816\n",
      "Batch 1541 \t loss=15.433597\n",
      "Batch 1561 \t loss=15.628569\n",
      "Batch 1581 \t loss=15.824302\n",
      "Batch 1601 \t loss=16.019526\n",
      "Batch 1621 \t loss=16.212128\n",
      "Batch 1641 \t loss=16.406383\n",
      "Batch 1661 \t loss=16.599872\n",
      "Batch 1681 \t loss=16.790652\n",
      "Batch 1701 \t loss=16.986553\n",
      "Batch 1721 \t loss=17.180361\n",
      "Batch 1741 \t loss=17.372985\n",
      "Batch 1761 \t loss=17.567745\n",
      "Batch 1781 \t loss=17.757532\n",
      "Batch 1801 \t loss=17.952547\n",
      "Batch 1821 \t loss=18.143057\n",
      "Batch 1841 \t loss=18.335476\n",
      "Batch 1861 \t loss=18.529124\n",
      "Batch 1881 \t loss=18.720894\n",
      "Batch 1901 \t loss=18.914219\n",
      "Batch 1921 \t loss=19.108295\n",
      "Batch 1941 \t loss=19.301638\n",
      "Batch 1961 \t loss=19.492501\n",
      "Batch 1981 \t loss=19.685337\n",
      "Batch 2001 \t loss=19.878320\n",
      "Batch 2021 \t loss=20.070254\n",
      "Batch 2041 \t loss=20.262349\n",
      "Batch 2061 \t loss=20.454369\n",
      "Batch 2081 \t loss=20.645560\n",
      "Batch 2101 \t loss=20.837102\n",
      "Batch 2121 \t loss=21.026294\n",
      "Batch 2141 \t loss=21.217385\n",
      "Batch 2161 \t loss=21.409084\n",
      "Batch 2181 \t loss=21.598595\n",
      "Batch 2201 \t loss=21.786247\n",
      "Batch 2221 \t loss=21.975327\n",
      "Batch 2241 \t loss=22.165286\n",
      "Batch 2261 \t loss=22.354836\n",
      "Batch 2281 \t loss=22.543630\n",
      "Batch 2301 \t loss=22.732733\n",
      "Batch 2321 \t loss=22.921387\n",
      "Batch 2341 \t loss=23.111670\n",
      "Batch 2361 \t loss=23.301477\n",
      "Batch 2381 \t loss=23.491471\n",
      "Batch 2401 \t loss=23.680252\n",
      "Batch 2421 \t loss=23.868926\n",
      "Batch 2441 \t loss=24.057073\n",
      "Batch 2461 \t loss=24.244691\n",
      "Batch 2481 \t loss=24.434032\n",
      "Batch 1 \t metric=0.000000\n",
      "Batch 21 \t metric=0.000000\n",
      "Batch 41 \t metric=0.000000\n",
      "Batch 61 \t metric=0.000000\n",
      "Batch 81 \t metric=0.000000\n",
      "Batch 101 \t metric=0.000000\n",
      "Batch 121 \t metric=0.000000\n",
      "Batch 141 \t metric=0.000000\n",
      "Batch 161 \t metric=0.000000\n",
      "Batch 181 \t metric=0.000000\n",
      "Batch 201 \t metric=0.000000\n",
      "Batch 221 \t metric=0.000000\n",
      "Batch 241 \t metric=0.000000\n",
      "Batch 261 \t metric=0.000000\n",
      "Batch 281 \t metric=0.000000\n",
      "Batch 301 \t metric=0.000000\n",
      "Batch 321 \t metric=0.000000\n",
      "Batch 341 \t metric=0.000000\n",
      "Batch 361 \t metric=0.000000\n",
      "Batch 381 \t metric=0.000000\n",
      "Batch 401 \t metric=0.000000\n",
      "Batch 421 \t metric=0.000000\n",
      "Batch 441 \t metric=0.000000\n",
      "Batch 461 \t metric=0.000000\n",
      "Batch 481 \t metric=0.000000\n",
      "Batch 501 \t metric=0.000000\n",
      "Batch 521 \t metric=0.000000\n",
      "Batch 541 \t metric=0.000000\n",
      "Batch 561 \t metric=0.000000\n",
      "Batch 581 \t metric=0.000000\n",
      "Batch 601 \t metric=0.000000\n",
      "Batch 621 \t metric=0.000000\n",
      "Epoch 1/5 \t loss=24.6006 \t metric=0.0000\n",
      "Batch 1 \t loss=0.009231\n",
      "Batch 21 \t loss=0.197497\n",
      "Batch 41 \t loss=0.385120\n",
      "Batch 61 \t loss=0.574335\n",
      "Batch 81 \t loss=0.762791\n",
      "Batch 101 \t loss=0.950059\n",
      "Batch 121 \t loss=1.138847\n",
      "Batch 141 \t loss=1.328382\n",
      "Batch 161 \t loss=1.515466\n",
      "Batch 181 \t loss=1.703069\n",
      "Batch 201 \t loss=1.889675\n",
      "Batch 221 \t loss=2.077597\n",
      "Batch 241 \t loss=2.264322\n",
      "Batch 261 \t loss=2.452735\n",
      "Batch 281 \t loss=2.641933\n",
      "Batch 301 \t loss=2.827816\n",
      "Batch 321 \t loss=3.012166\n",
      "Batch 341 \t loss=3.200668\n",
      "Batch 361 \t loss=3.388146\n",
      "Batch 381 \t loss=3.573052\n",
      "Batch 401 \t loss=3.759246\n",
      "Batch 421 \t loss=3.943720\n",
      "Batch 441 \t loss=4.127825\n",
      "Batch 461 \t loss=4.313509\n",
      "Batch 481 \t loss=4.498736\n",
      "Batch 501 \t loss=4.684139\n",
      "Batch 521 \t loss=4.869377\n",
      "Batch 541 \t loss=5.054977\n",
      "Batch 561 \t loss=5.240173\n",
      "Batch 581 \t loss=5.425953\n",
      "Batch 601 \t loss=5.611301\n",
      "Batch 621 \t loss=5.795717\n",
      "Batch 641 \t loss=5.981713\n",
      "Batch 661 \t loss=6.165191\n",
      "Batch 681 \t loss=6.349952\n",
      "Batch 701 \t loss=6.533432\n",
      "Batch 721 \t loss=6.717759\n",
      "Batch 741 \t loss=6.902523\n",
      "Batch 761 \t loss=7.086783\n",
      "Batch 781 \t loss=7.272464\n",
      "Batch 801 \t loss=7.456583\n",
      "Batch 821 \t loss=7.641125\n",
      "Batch 841 \t loss=7.826824\n",
      "Batch 861 \t loss=8.010934\n",
      "Batch 881 \t loss=8.196592\n",
      "Batch 901 \t loss=8.379280\n",
      "Batch 921 \t loss=8.562843\n",
      "Batch 941 \t loss=8.746691\n",
      "Batch 961 \t loss=8.929513\n",
      "Batch 981 \t loss=9.113050\n",
      "Batch 1001 \t loss=9.296071\n",
      "Batch 1021 \t loss=9.478528\n",
      "Batch 1041 \t loss=9.662053\n",
      "Batch 1061 \t loss=9.844923\n",
      "Batch 1081 \t loss=10.028541\n",
      "Batch 1101 \t loss=10.213345\n",
      "Batch 1121 \t loss=10.396251\n",
      "Batch 1141 \t loss=10.578810\n",
      "Batch 1161 \t loss=10.761600\n",
      "Batch 1181 \t loss=10.945115\n",
      "Batch 1201 \t loss=11.128272\n",
      "Batch 1221 \t loss=11.311495\n",
      "Batch 1241 \t loss=11.494145\n",
      "Batch 1261 \t loss=11.676344\n",
      "Batch 1281 \t loss=11.858537\n",
      "Batch 1301 \t loss=12.040611\n",
      "Batch 1321 \t loss=12.222843\n",
      "Batch 1341 \t loss=12.405270\n",
      "Batch 1361 \t loss=12.586617\n",
      "Batch 1381 \t loss=12.767839\n",
      "Batch 1401 \t loss=12.950252\n",
      "Batch 1421 \t loss=13.132345\n",
      "Batch 1441 \t loss=13.314724\n",
      "Batch 1461 \t loss=13.496827\n",
      "Batch 1481 \t loss=13.678223\n",
      "Batch 1501 \t loss=13.859698\n",
      "Batch 1521 \t loss=14.041749\n",
      "Batch 1541 \t loss=14.224582\n",
      "Batch 1561 \t loss=14.405959\n",
      "Batch 1581 \t loss=14.587279\n",
      "Batch 1601 \t loss=14.769278\n",
      "Batch 1621 \t loss=14.950789\n",
      "Batch 1641 \t loss=15.131724\n",
      "Batch 1661 \t loss=15.313319\n",
      "Batch 1681 \t loss=15.495375\n",
      "Batch 1701 \t loss=15.676695\n",
      "Batch 1721 \t loss=15.857877\n",
      "Batch 1741 \t loss=16.039123\n",
      "Batch 1761 \t loss=16.220886\n",
      "Batch 1781 \t loss=16.402450\n",
      "Batch 1801 \t loss=16.583526\n",
      "Batch 1821 \t loss=16.764033\n",
      "Batch 1841 \t loss=16.945004\n",
      "Batch 1861 \t loss=17.126305\n",
      "Batch 1881 \t loss=17.307249\n",
      "Batch 1901 \t loss=17.488103\n",
      "Batch 1921 \t loss=17.669031\n",
      "Batch 1941 \t loss=17.849709\n",
      "Batch 1961 \t loss=18.030706\n",
      "Batch 1981 \t loss=18.210905\n",
      "Batch 2001 \t loss=18.391513\n",
      "Batch 2021 \t loss=18.572144\n",
      "Batch 2041 \t loss=18.753470\n",
      "Batch 2061 \t loss=18.933883\n",
      "Batch 2081 \t loss=19.114530\n",
      "Batch 2101 \t loss=19.294768\n",
      "Batch 2121 \t loss=19.475407\n",
      "Batch 2141 \t loss=19.655247\n",
      "Batch 2161 \t loss=19.835663\n",
      "Batch 2181 \t loss=20.015614\n",
      "Batch 2201 \t loss=20.195965\n",
      "Batch 2221 \t loss=20.376547\n",
      "Batch 2241 \t loss=20.556768\n",
      "Batch 2261 \t loss=20.737225\n",
      "Batch 2281 \t loss=20.917437\n",
      "Batch 2301 \t loss=21.097634\n",
      "Batch 2321 \t loss=21.278275\n",
      "Batch 2341 \t loss=21.457596\n",
      "Batch 2361 \t loss=21.637711\n",
      "Batch 2381 \t loss=21.817472\n",
      "Batch 2401 \t loss=21.997679\n",
      "Batch 2421 \t loss=22.177766\n",
      "Batch 2441 \t loss=22.357549\n",
      "Batch 2461 \t loss=22.537370\n",
      "Batch 2481 \t loss=22.717226\n",
      "Batch 1 \t metric=0.000000\n",
      "Batch 21 \t metric=0.000000\n",
      "Batch 41 \t metric=0.000000\n",
      "Batch 61 \t metric=0.000000\n",
      "Batch 81 \t metric=0.000000\n",
      "Batch 101 \t metric=0.000000\n",
      "Batch 121 \t metric=0.000000\n",
      "Batch 141 \t metric=0.000000\n",
      "Batch 161 \t metric=0.000000\n",
      "Batch 181 \t metric=0.000000\n",
      "Batch 201 \t metric=0.000000\n",
      "Batch 221 \t metric=0.000000\n",
      "Batch 241 \t metric=0.000000\n",
      "Batch 261 \t metric=0.000000\n",
      "Batch 281 \t metric=0.000000\n",
      "Batch 301 \t metric=0.000000\n",
      "Batch 321 \t metric=0.000000\n",
      "Batch 341 \t metric=0.000000\n",
      "Batch 361 \t metric=0.000000\n",
      "Batch 381 \t metric=0.000000\n",
      "Batch 401 \t metric=0.000000\n",
      "Batch 421 \t metric=0.000000\n",
      "Batch 441 \t metric=0.000000\n",
      "Batch 461 \t metric=0.000000\n",
      "Batch 481 \t metric=0.000000\n",
      "Batch 501 \t metric=0.000000\n",
      "Batch 521 \t metric=0.000000\n",
      "Batch 541 \t metric=0.000000\n",
      "Batch 561 \t metric=0.000000\n",
      "Batch 581 \t metric=0.000000\n",
      "Batch 601 \t metric=0.000000\n",
      "Batch 621 \t metric=0.000000\n",
      "Epoch 2/5 \t loss=22.8786 \t metric=0.0000\n",
      "Batch 1 \t loss=0.008948\n",
      "Batch 21 \t loss=0.188561\n",
      "Batch 41 \t loss=0.368318\n",
      "Batch 61 \t loss=0.548252\n",
      "Batch 81 \t loss=0.727713\n",
      "Batch 101 \t loss=0.907384\n",
      "Batch 121 \t loss=1.086614\n",
      "Batch 141 \t loss=1.266399\n",
      "Batch 161 \t loss=1.445457\n",
      "Batch 181 \t loss=1.624737\n",
      "Batch 201 \t loss=1.803883\n",
      "Batch 221 \t loss=1.983605\n",
      "Batch 241 \t loss=2.163559\n",
      "Batch 261 \t loss=2.342638\n",
      "Batch 281 \t loss=2.522338\n",
      "Batch 301 \t loss=2.701877\n",
      "Batch 321 \t loss=2.881502\n",
      "Batch 341 \t loss=3.061133\n",
      "Batch 361 \t loss=3.240391\n",
      "Batch 381 \t loss=3.420048\n",
      "Batch 401 \t loss=3.599568\n",
      "Batch 421 \t loss=3.779362\n",
      "Batch 441 \t loss=3.958388\n",
      "Batch 461 \t loss=4.137501\n",
      "Batch 481 \t loss=4.317002\n",
      "Batch 501 \t loss=4.496258\n",
      "Batch 521 \t loss=4.675746\n",
      "Batch 541 \t loss=4.855245\n",
      "Batch 561 \t loss=5.034328\n",
      "Batch 581 \t loss=5.213357\n",
      "Batch 601 \t loss=5.392660\n",
      "Batch 621 \t loss=5.571839\n",
      "Batch 641 \t loss=5.750999\n",
      "Batch 661 \t loss=5.930605\n",
      "Batch 681 \t loss=6.109530\n",
      "Batch 701 \t loss=6.288369\n",
      "Batch 721 \t loss=6.467158\n",
      "Batch 741 \t loss=6.646042\n",
      "Batch 761 \t loss=6.825188\n",
      "Batch 781 \t loss=7.003783\n",
      "Batch 801 \t loss=7.182716\n",
      "Batch 821 \t loss=7.361682\n",
      "Batch 841 \t loss=7.540757\n",
      "Batch 861 \t loss=7.719751\n",
      "Batch 881 \t loss=7.898437\n",
      "Batch 901 \t loss=8.077406\n",
      "Batch 921 \t loss=8.256434\n",
      "Batch 941 \t loss=8.435767\n",
      "Batch 961 \t loss=8.614814\n",
      "Batch 981 \t loss=8.793464\n",
      "Batch 1001 \t loss=8.972356\n",
      "Batch 1021 \t loss=9.151240\n",
      "Batch 1041 \t loss=9.330394\n",
      "Batch 1061 \t loss=9.509112\n",
      "Batch 1081 \t loss=9.687412\n",
      "Batch 1101 \t loss=9.866209\n",
      "Batch 1121 \t loss=10.045351\n",
      "Batch 1141 \t loss=10.223822\n",
      "Batch 1161 \t loss=10.402122\n",
      "Batch 1181 \t loss=10.580883\n",
      "Batch 1201 \t loss=10.759707\n",
      "Batch 1221 \t loss=10.938221\n",
      "Batch 1241 \t loss=11.116857\n",
      "Batch 1261 \t loss=11.295864\n",
      "Batch 1281 \t loss=11.474418\n",
      "Batch 1301 \t loss=11.653093\n",
      "Batch 1321 \t loss=11.831744\n",
      "Batch 1341 \t loss=12.010047\n",
      "Batch 1361 \t loss=12.188710\n",
      "Batch 1381 \t loss=12.367301\n",
      "Batch 1401 \t loss=12.545564\n",
      "Batch 1421 \t loss=12.723728\n",
      "Batch 1441 \t loss=12.902117\n",
      "Batch 1461 \t loss=13.080701\n",
      "Batch 1481 \t loss=13.259268\n",
      "Batch 1501 \t loss=13.437663\n",
      "Batch 1521 \t loss=13.616197\n",
      "Batch 1541 \t loss=13.794536\n",
      "Batch 1561 \t loss=13.973121\n",
      "Batch 1581 \t loss=14.151845\n",
      "Batch 1601 \t loss=14.330289\n",
      "Batch 1621 \t loss=14.508511\n",
      "Batch 1641 \t loss=14.687068\n",
      "Batch 1661 \t loss=14.865271\n",
      "Batch 1681 \t loss=15.043638\n",
      "Batch 1701 \t loss=15.221882\n",
      "Batch 1721 \t loss=15.400326\n",
      "Batch 1741 \t loss=15.578645\n",
      "Batch 1761 \t loss=15.757279\n",
      "Batch 1781 \t loss=15.935562\n",
      "Batch 1801 \t loss=16.113725\n",
      "Batch 1821 \t loss=16.292241\n",
      "Batch 1841 \t loss=16.470431\n",
      "Batch 1861 \t loss=16.648895\n",
      "Batch 1881 \t loss=16.827197\n",
      "Batch 1901 \t loss=17.005497\n",
      "Batch 1921 \t loss=17.183836\n",
      "Batch 1941 \t loss=17.362159\n",
      "Batch 1961 \t loss=17.540400\n",
      "Batch 1981 \t loss=17.718714\n",
      "Batch 2001 \t loss=17.897001\n",
      "Batch 2021 \t loss=18.075123\n",
      "Batch 2041 \t loss=18.253434\n",
      "Batch 2061 \t loss=18.431633\n",
      "Batch 2081 \t loss=18.609761\n",
      "Batch 2101 \t loss=18.787966\n",
      "Batch 2121 \t loss=18.965982\n",
      "Batch 2141 \t loss=19.144298\n",
      "Batch 2161 \t loss=19.322398\n",
      "Batch 2181 \t loss=19.500430\n",
      "Batch 2201 \t loss=19.678734\n",
      "Batch 2221 \t loss=19.857021\n",
      "Batch 2241 \t loss=20.035230\n",
      "Batch 2261 \t loss=20.213172\n",
      "Batch 2281 \t loss=20.391350\n",
      "Batch 2301 \t loss=20.569430\n",
      "Batch 2321 \t loss=20.747691\n",
      "Batch 2341 \t loss=20.925904\n",
      "Batch 2361 \t loss=21.103929\n",
      "Batch 2381 \t loss=21.281891\n",
      "Batch 2401 \t loss=21.460508\n",
      "Batch 2421 \t loss=21.638785\n",
      "Batch 2441 \t loss=21.816766\n",
      "Batch 2461 \t loss=21.995139\n",
      "Batch 2481 \t loss=22.173137\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [91]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (n \u001b[38;5;241m%\u001b[39m Batch_size \u001b[38;5;241m==\u001b[39m Batch_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train_1)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     24\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 25\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m num_train_batch\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_num \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\bd4hproject\\lib\\site-packages\\torch\\autograd\\grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[1;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\bd4hproject\\lib\\site-packages\\torch\\optim\\adam.py:108\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    107\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 108\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m           \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\bd4hproject\\lib\\site-packages\\torch\\optim\\functional.py:87\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m     86\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m---> 87\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmaximum(max_exp_avg_sq, exp_avg_sq, out\u001b[38;5;241m=\u001b[39mmax_exp_avg_sq)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):  \n",
    "    # Train\n",
    "    LSTMmodel.train()\n",
    "    X_train_1, X_train_2, Y_train = shuffle(X_train_1, X_train_2, Y_train)\n",
    "    \n",
    "    avg_loss = 0. \n",
    "    batch_num = 1\n",
    "    for n in range(len(X_train_1)):\n",
    "        # Step 0. Split the batch\n",
    "        \n",
    "        if n % Batch_size == 0:\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            LSTMmodel.zero_grad()\n",
    "            loss = 0\n",
    "        # Step 2. Run our forward pass.\n",
    "        tag_scores = LSTMmodel(X_train_1[n], X_train_2[n])\n",
    "\n",
    "        # Step 3. Compute the loss\n",
    "        loss += loss_fn(tag_scores[:,-1], Y_train[n])\n",
    "        \n",
    "        # Step 4. Compute gradients, and update the parameters by calling optimizer.step()\n",
    "        if (n % Batch_size == Batch_size - 1) or n == len(X_train_1)-1:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / num_train_batch\n",
    "            if batch_num % 20 == 1:\n",
    "                print('Batch {} \\t loss={:4f}'.format(batch_num, avg_loss))\n",
    "            batch_num += 1\n",
    "        \n",
    "    # Evaluation\n",
    "    LSTMmodel.eval()\n",
    "    X_valid_1, X_valid_2, Y_valid = shuffle(X_valid_1, X_valid_2, Y_valid)\n",
    "    \n",
    "    batch_num = 1\n",
    "    avg_metric = 0.\n",
    "    for n in range(len(X_valid_1)):\n",
    "        if n % Batch_size == 0:\n",
    "            predict_scores, real_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            tag_scores = LSTMmodel(X_valid_1[n], X_valid_2[n])\n",
    "            \n",
    "        predict_label = np.argmax(tag_scores.detach().numpy(), axis=1).flatten()\n",
    "        predict_scores.append(predict_label)\n",
    "        real_labels.append(Y_valid[n].numpy())\n",
    "        if (n % Batch_size == Batch_size - 1) or n == len(X_valid_1)-1:\n",
    "            avg_metric += f1_score(predict_scores, real_labels) / num_valid_batch\n",
    "            if batch_num % 20 == 1:\n",
    "                print('Batch {} \\t metric={:4f}'.format(batch_num, avg_metric))\n",
    "            batch_num += 1\n",
    "        \n",
    "    # Output\n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t metric={:.4f}'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, avg_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8c9f5270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def save_ckp(state, is_best, checkpoint_dir=\"\", best_model_dir=\"\"):\n",
    "    f_path = 'checkpoint.pt'\n",
    "    torch.save(state, f_path)\n",
    "    if is_best:\n",
    "        best_fpath =  'best_model.pt'\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "70c71710",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'epoch': epoch + 1,'loss': avg_loss, 'metric': avg_metric, 'state_dict': LSTMmodel.state_dict(),'optimizer': optimizer.state_dict()}\n",
    "        \n",
    "# save checkpoint\n",
    "save_ckp(checkpoint, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4948f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 \t metric=0.000000\n",
      "Batch 21 \t metric=0.000000\n",
      "Batch 41 \t metric=0.000000\n",
      "Batch 61 \t metric=0.000000\n",
      "Batch 81 \t metric=0.000000\n",
      "Batch 101 \t metric=0.000000\n",
      "Batch 121 \t metric=0.000000\n",
      "Batch 141 \t metric=0.000000\n",
      "Batch 161 \t metric=0.000000\n",
      "Batch 181 \t metric=0.000000\n",
      "Batch 201 \t metric=0.000000\n",
      "Batch 221 \t metric=0.000000\n",
      "Batch 241 \t metric=0.000000\n",
      "Batch 261 \t metric=0.000000\n",
      "Batch 281 \t metric=0.000000\n",
      "Batch 301 \t metric=0.000000\n",
      "Batch 321 \t metric=0.000000\n",
      "Batch 341 \t metric=0.000000\n",
      "Batch 361 \t metric=0.000000\n",
      "Batch 381 \t metric=0.000000\n",
      "Batch 401 \t metric=0.000000\n",
      "Batch 421 \t metric=0.000000\n",
      "Batch 441 \t metric=0.000000\n",
      "Batch 461 \t metric=0.000000\n",
      "Batch 481 \t metric=0.000000\n",
      "Batch 501 \t metric=0.000000\n",
      "Batch 521 \t metric=0.000000\n",
      "Batch 541 \t metric=0.000000\n",
      "Batch 561 \t metric=0.000000\n",
      "Batch 581 \t metric=0.000000\n",
      "Batch 601 \t metric=0.000000\n",
      "Batch 621 \t metric=0.000000\n"
     ]
    }
   ],
   "source": [
    "    # Evaluation\n",
    "    LSTMmodel.eval()\n",
    "    X_valid_1, X_valid_2, Y_valid = shuffle(X_valid_1, X_valid_2, Y_valid)\n",
    "    \n",
    "    batch_num = 1\n",
    "    avg_metric = 0.\n",
    "    for n in range(len(X_valid_1)):\n",
    "        if n % Batch_size == 0:\n",
    "            predict_scores, real_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            tag_scores = LSTMmodel(X_valid_1[n], X_valid_2[n])\n",
    "            \n",
    "        predict_label = np.argmax(tag_scores.detach().numpy(), axis=1).flatten()\n",
    "        predict_scores.append(predict_label)\n",
    "        real_labels.append(Y_valid[n].numpy())\n",
    "        if (n % Batch_size == Batch_size - 1) or n == len(X_valid_1)-1:\n",
    "            avg_metric += f1_score(predict_scores, real_labels) / num_valid_batch\n",
    "            if batch_num % 20 == 1:\n",
    "                print('Batch {} \\t metric={:4f}'.format(batch_num, avg_metric))\n",
    "            batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b9328ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9985e-01, 1.5031e-04]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea5690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
